{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert given source yaml data to csv file\n",
    "import yaml\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def yaml_to_csv_recursive(base_input_dir, base_output_dir):\n",
    "    base_input_path = Path(base_input_dir)\n",
    "    base_output_path = Path(base_output_dir)\n",
    "    \n",
    "    base_output_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    for root, dirs, files in os.walk(base_input_path):\n",
    "        current_path = Path(root)\n",
    "        \n",
    "        rel_path = current_path.relative_to(base_input_path)\n",
    "        \n",
    "        output_dir = base_output_path / rel_path\n",
    "        output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        for file in files:\n",
    "            if file.endswith(('.yml', '.yaml')):\n",
    "                input_file = current_path / file\n",
    "                output_file = output_dir / f\"{file.rsplit('.', 1)[0]}.csv\"\n",
    "                \n",
    "                try:\n",
    "                    with open(input_file, 'r', encoding='utf-8') as yaml_file:\n",
    "                        yaml_data = yaml.safe_load(yaml_file)\n",
    "                    \n",
    "                    if isinstance(yaml_data, list):\n",
    "                        df = pd.DataFrame(yaml_data)\n",
    "                    else:\n",
    "                        df = pd.DataFrame([yaml_data])\n",
    "                    \n",
    "                   \n",
    "                    df.to_csv(output_file, index=False)\n",
    "                    print(f\"Converted: {input_file} -> {output_file}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {input_file}: {str(e)}\")\n",
    "\n",
    "\n",
    "input_directory = r\"D:\\Stock Project\\data\"  \n",
    "output_directory = r\"D:\\Stock Project\\data YAML to CSV\"  \n",
    "yaml_to_csv_recursive(input_directory, output_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seperate csv files based on company name\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "import glob\n",
    "\n",
    "def split_by_ticker(input_directory, output_directory):\n",
    "    \n",
    "    output_path = Path(output_directory)\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    csv_files = glob.glob(str(Path(input_directory) / \"**\" / \"*.csv\"), recursive=True)\n",
    "    \n",
    "    print(\"Reading CSV files...\")\n",
    "    all_data = []\n",
    "    for file in csv_files:\n",
    "        try:\n",
    "            df = pd.read_csv(file)\n",
    "            all_data.append(df)\n",
    "            print(f\"Read: {file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file}: {str(e)}\")\n",
    "\n",
    "    print(\"\\nCombining data...\")\n",
    "    combined_df = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "    tickers = combined_df['Ticker'].unique()\n",
    "    print(f\"\\nFound {len(tickers)} unique tickers\")\n",
    "\n",
    "    print(\"\\nSplitting and saving files...\")\n",
    "    for ticker in tickers:\n",
    "        ticker_data = combined_df[combined_df['Ticker'] == ticker]\n",
    "\n",
    "        filename = f\"{ticker}.csv\"\n",
    "        output_file = output_path / filename\n",
    "\n",
    "        ticker_data.to_csv(output_file, index=False)\n",
    "        print(f\"Saved {filename} with {len(ticker_data)} rows\")\n",
    "\n",
    "    print(\"\\nProcessing complete!\")\n",
    "    print(f\"Total number of tickers processed: {len(tickers)}\")\n",
    "\n",
    "input_directory = r\"D:\\Stock Project\\data YAML to CSV\"\n",
    "output_directory = r\"D:\\Stock Project\\split_by_ticker\"\n",
    "\n",
    "split_by_ticker(input_directory, output_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Top 10 Most Volatile Stocks from November 2023 - November 2024\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import glob\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def calculate_volatility(input_directory, output_directory):\n",
    "    output_path = Path(output_directory)\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    start_date = datetime(2023, 11, 1)     \n",
    "    end_date = datetime(2024, 11, 30)      \n",
    "\n",
    "    csv_files = glob.glob(str(Path(input_directory) / \"*.csv\"))\n",
    "    print(f\"Found {len(csv_files)} CSV files\")\n",
    "    print(f\"Analyzing period: {start_date.strftime('%B %Y')} - {end_date.strftime('%B %Y')}\")\n",
    "\n",
    "    volatility_results = []\n",
    "    \n",
    "    print(\"\\nProcessing files...\")\n",
    "    for file in csv_files:\n",
    "        try:\n",
    "            df = pd.read_csv(file)\n",
    "\n",
    "            df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "            mask = (df['date'] >= start_date) & (df['date'] <= end_date)\n",
    "            df = df[mask]\n",
    "\n",
    "            if len(df) < 20: \n",
    "                print(f\"Skipping {Path(file).stem} - insufficient data points\")\n",
    "                continue\n",
    "\n",
    "            df = df.sort_values('date')\n",
    "\n",
    "            daily_returns = (df['close'] - df['close'].shift(1)) / df['close'].shift(1)\n",
    "\n",
    "            std_dev = daily_returns.std()\n",
    "\n",
    "            annualized_vol = std_dev * (252 ** 0.5) \n",
    "\n",
    "            ticker = df['Ticker'].iloc[0]\n",
    "            \n",
    "            volatility_results.append({\n",
    "                'Ticker': ticker,\n",
    "                'Daily_StdDev': std_dev,  \n",
    "                'Annualized_Volatility': annualized_vol,  \n",
    "                'Data_Points': len(df),\n",
    "                'Avg_Daily_Return': daily_returns.mean(),\n",
    "                'Max_Daily_Return': daily_returns.max(),\n",
    "                'Min_Daily_Return': daily_returns.min(),\n",
    "                'First_Date': df['date'].min().strftime('%Y-%m-%d'),\n",
    "                'Last_Date': df['date'].max().strftime('%Y-%m-%d'),\n",
    "                'Start_Price': df['close'].iloc[0],\n",
    "                'End_Price': df['close'].iloc[-1]\n",
    "            })\n",
    "            \n",
    "            print(f\"Processed {ticker} with {len(df)} trading days\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file}: {str(e)}\")\n",
    "\n",
    "    volatility_df = pd.DataFrame(volatility_results)\n",
    "\n",
    "    top_10_volatile = volatility_df.nlargest(10, 'Annualized_Volatility')\n",
    "\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    bars = plt.bar(top_10_volatile['Ticker'], top_10_volatile['Annualized_Volatility'])\n",
    "    \n",
    "    plt.title('Top 10 Most Volatile Stocks\\n'\n",
    "              f'Period: November 2023 - November 2024', pad=20)\n",
    "    plt.xlabel('Stock Ticker')\n",
    "    plt.ylabel('Annualized Volatility')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.1%}',\n",
    "                ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig(output_path / 'top_10_volatility.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"\\nVisualization saved as 'top_10_volatility.png'\")\n",
    "\n",
    "    csv_df = volatility_df.copy()\n",
    "    for col in ['Daily_StdDev', 'Annualized_Volatility', 'Avg_Daily_Return', 'Max_Daily_Return', 'Min_Daily_Return']:\n",
    "        csv_df[col] = csv_df[col].map('{:.6f}'.format)\n",
    "\n",
    "    csv_df.sort_values('Annualized_Volatility', ascending=False).to_csv(\n",
    "        output_path / 'volatility_analysis.csv', index=False\n",
    "    )\n",
    "    print(\"Complete volatility data saved as 'volatility_analysis.csv'\")\n",
    "\n",
    "    print(\"\\nVolatility Analysis Summary (November 2023 - November 2024):\")\n",
    "    print(\"\\nTop 10 Most Volatile Stocks:\")\n",
    "    print(\"=\" * 80)\n",
    "    for _, row in top_10_volatile.iterrows():\n",
    "        print(f\"Ticker: {row['Ticker']}\")\n",
    "        print(f\"Daily Standard Deviation: {row['Daily_StdDev']:.2%}\")\n",
    "        print(f\"Annualized Volatility: {row['Annualized_Volatility']:.2%}\")\n",
    "        print(f\"Average Daily Return: {row['Avg_Daily_Return']:.2%}\")\n",
    "        print(f\"Maximum Daily Return: {row['Max_Daily_Return']:.2%}\")\n",
    "        print(f\"Minimum Daily Return: {row['Min_Daily_Return']:.2%}\")\n",
    "        print(f\"Trading Days: {row['Data_Points']}\")\n",
    "        print(f\"Date Range: {row['First_Date']} to {row['Last_Date']}\")\n",
    "        print(f\"Price Range: {row['Start_Price']:.2f} to {row['End_Price']:.2f}\")\n",
    "        print(\"-\" * 80)\n",
    "    \n",
    "    print(\"\\nOverall Market Statistics:\")\n",
    "    print(f\"Average Daily StdDev: {volatility_df['Daily_StdDev'].mean():.2%}\")\n",
    "    print(f\"Average Annualized Volatility: {volatility_df['Annualized_Volatility'].mean():.2%}\")\n",
    "    print(f\"Most Volatile Stock: {volatility_df.iloc[0]['Ticker']} ({volatility_df['Annualized_Volatility'].max():.2%})\")\n",
    "    print(f\"Least Volatile Stock: {volatility_df.iloc[-1]['Ticker']} ({volatility_df['Annualized_Volatility'].min():.2%})\")\n",
    "    print(f\"Average Trading Days: {volatility_df['Data_Points'].mean():.0f}\")\n",
    "    \n",
    "    return volatility_df\n",
    "\n",
    "input_directory = r\"D:\\Stock Project\\split_by_ticker\"\n",
    "output_directory = r\"D:\\Stock Project\\volatility_analysis\"\n",
    "\n",
    "volatility_results = calculate_volatility(input_directory, output_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cumulative Returns of Top 5 Performing Stocks from November 2023 - November 2024\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import glob\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def analyze_cumulative_returns(input_directory, output_directory):\n",
    "    output_path = Path(output_directory)\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    start_date = datetime(2023, 11, 1)     \n",
    "    end_date = datetime(2024, 11, 30)      \n",
    "\n",
    "    csv_files = glob.glob(str(Path(input_directory) / \"*.csv\"))\n",
    "    print(f\"Found {len(csv_files)} CSV files\")\n",
    "    print(f\"Analyzing period: {start_date.strftime('%B %Y')} - {end_date.strftime('%B %Y')}\")\n",
    "\n",
    "    stock_returns = {}\n",
    "    cumulative_returns = []\n",
    "    \n",
    "    print(\"\\nProcessing files...\")\n",
    "    for file in csv_files:\n",
    "        try:\n",
    "            df = pd.read_csv(file)\n",
    "\n",
    "            df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "            mask = (df['date'] >= start_date) & (df['date'] <= end_date)\n",
    "            df = df[mask]\n",
    "\n",
    "            if len(df) < 20:  \n",
    "                print(f\"Skipping {Path(file).stem} - insufficient data points\")\n",
    "                continue\n",
    "\n",
    "            df = df.sort_values('date')\n",
    "\n",
    "            df['daily_return'] = df['close'].pct_change()\n",
    "\n",
    "            df['cumulative_return'] = (1 + df['daily_return']).cumprod() - 1\n",
    "\n",
    "            ticker = df['Ticker'].iloc[0]\n",
    "\n",
    "            stock_returns[ticker] = df[['date', 'cumulative_return']].copy()\n",
    "\n",
    "            final_return = df['cumulative_return'].iloc[-1]\n",
    "            cumulative_returns.append({\n",
    "                'Ticker': ticker,\n",
    "                'Cumulative_Return': final_return,\n",
    "                'Start_Price': df['close'].iloc[0],\n",
    "                'End_Price': df['close'].iloc[-1],\n",
    "                'Trading_Days': len(df)\n",
    "            })\n",
    "            \n",
    "            print(f\"Processed {ticker} with {len(df)} trading days\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file}: {str(e)}\")\n",
    "\n",
    "    returns_df = pd.DataFrame(cumulative_returns)\n",
    "    top_5_performers = returns_df.nlargest(5, 'Cumulative_Return')\n",
    "\n",
    "    plt.figure(figsize=(15, 8))\n",
    "\n",
    "    for _, row in top_5_performers.iterrows():\n",
    "        ticker = row['Ticker']\n",
    "        data = stock_returns[ticker]\n",
    "        plt.plot(data['date'], data['cumulative_return'] * 100, label=f\"{ticker} ({row['Cumulative_Return']:.1%})\")\n",
    "    \n",
    "    plt.title('Cumulative Returns of Top 5 Performing Stocks\\n'\n",
    "              f'Period: November 2023 - November 2024', pad=20)\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Cumulative Return (%)')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend(title='Stock (Total Return)', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig(output_path / 'top_5_cumulative_returns.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"\\nVisualization saved as 'top_5_cumulative_returns.png'\")\n",
    "\n",
    "    returns_df.sort_values('Cumulative_Return', ascending=False).to_csv(\n",
    "        output_path / 'cumulative_returns_analysis.csv', index=False\n",
    "    )\n",
    "    print(\"Complete returns data saved as 'cumulative_returns_analysis.csv'\")\n",
    "\n",
    "    print(\"\\nTop 5 Performing Stocks:\")\n",
    "    print(\"=\" * 80)\n",
    "    for _, row in top_5_performers.iterrows():\n",
    "        print(f\"Ticker: {row['Ticker']}\")\n",
    "        print(f\"Cumulative Return: {row['Cumulative_Return']:.2%}\")\n",
    "        print(f\"Start Price: {row['Start_Price']:.2f}\")\n",
    "        print(f\"End Price: {row['End_Price']:.2f}\")\n",
    "        print(f\"Trading Days: {row['Trading_Days']}\")\n",
    "        print(\"-\" * 80)\n",
    "    \n",
    "    print(\"\\nOverall Market Statistics:\")\n",
    "    print(f\"Average Return: {returns_df['Cumulative_Return'].mean():.2%}\")\n",
    "    print(f\"Median Return: {returns_df['Cumulative_Return'].median():.2%}\")\n",
    "    print(f\"Best Performer: {returns_df.iloc[0]['Ticker']} ({returns_df['Cumulative_Return'].max():.2%})\")\n",
    "    print(f\"Worst Performer: {returns_df.iloc[-1]['Ticker']} ({returns_df['Cumulative_Return'].min():.2%})\")\n",
    "    \n",
    "    return returns_df\n",
    "\n",
    "input_directory = r\"D:\\Stock Project\\split_by_ticker\"\n",
    "output_directory = r\"D:\\Stock Project\\returns_analysis\"\n",
    "\n",
    "returns_results = analyze_cumulative_returns(input_directory, output_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sector-wise Performance from November 2023 - November 2024\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import glob\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def analyze_sector_performance(input_directory, output_directory):\n",
    "    output_path = Path(output_directory)\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    start_date = datetime(2023, 11, 1)\n",
    "    end_date = datetime(2024, 11, 30)\n",
    "\n",
    "    sector_mapping = {\n",
    "        'ADANIENT': 'Infrastructure',\n",
    "        'ADANIPORTS': 'Infrastructure',\n",
    "        'APOLLOHOSP': 'Healthcare',\n",
    "        'ASIANPAINT': 'Consumer Goods',\n",
    "        'AXISBANK': 'Banking',\n",
    "        'BAJAJ-AUTO': 'Automobile',\n",
    "        'BAJAJFINSV': 'Financial Services',\n",
    "        'BAJFINANCE': 'Financial Services',\n",
    "        'BEL': 'Defense',\n",
    "        'BHARTIARTL': 'Telecom',\n",
    "        'BPCL': 'Oil & Gas',\n",
    "        'BRITANNIA': 'FMCG',\n",
    "        'CIPLA': 'Healthcare',\n",
    "        'COALINDIA': 'Mining',\n",
    "        'DRREDDY': 'Healthcare',\n",
    "        'EICHERMOT': 'Automobile',\n",
    "        'GRASIM': 'Cement',\n",
    "        'HCLTECH': 'IT',\n",
    "        'HDFCBANK': 'Banking',\n",
    "        'HDFCLIFE': 'Insurance',\n",
    "        'HEROMOTOCO': 'Automobile',\n",
    "        'HINDALCO': 'Metal',\n",
    "        'HINDUNILVR': 'FMCG',\n",
    "        'ICICIBANK': 'Banking',\n",
    "        'INDUSINDBK': 'Banking',\n",
    "        'INFY': 'IT',\n",
    "        'ITC': 'FMCG',\n",
    "        'JSWSTEEL': 'Metal',\n",
    "        'KOTAKBANK': 'Banking',\n",
    "        'LT': 'Infrastructure',\n",
    "        'M&M': 'Automobile',\n",
    "        'MARUTI': 'Automobile',\n",
    "        'NESTLEIND': 'FMCG',\n",
    "        'NTPC': 'Power',\n",
    "        'ONGC': 'Oil & Gas',\n",
    "        'POWERGRID': 'Power',\n",
    "        'RELIANCE': 'Conglomerate',\n",
    "        'SBILIFE': 'Insurance',\n",
    "        'SBIN': 'Banking',\n",
    "        'SHRIRAMFIN': 'Financial Services',\n",
    "        'SUNPHARMA': 'Healthcare',\n",
    "        'TATACONSUM': 'FMCG',\n",
    "        'TATAMOTORS': 'Automobile',\n",
    "        'TATASTEEL': 'Metal',\n",
    "        'TCS': 'IT',\n",
    "        'TECHM': 'IT',\n",
    "        'TITAN': 'Consumer Goods',\n",
    "        'TRENT': 'Retail',\n",
    "        'ULTRACEMCO': 'Cement',\n",
    "        'WIPRO': 'IT'\n",
    "    }\n",
    "\n",
    "    sector_data = {}\n",
    "    \n",
    "    print(f\"Analyzing period: {start_date.strftime('%B %Y')} - {end_date.strftime('%B %Y')}\")\n",
    "\n",
    "    for file in glob.glob(str(Path(input_directory) / \"*.csv\")):\n",
    "        try:\n",
    "            df = pd.read_csv(file)\n",
    "            ticker = df['Ticker'].iloc[0]\n",
    "\n",
    "            if ticker not in sector_mapping:\n",
    "                print(f\"Warning: No sector mapping for {ticker}\")\n",
    "                continue\n",
    "                \n",
    "            sector = sector_mapping[ticker]\n",
    "\n",
    "            df['date'] = pd.to_datetime(df['date'])\n",
    "            df = df[(df['date'] >= start_date) & (df['date'] <= end_date)]\n",
    "            \n",
    "            if len(df) < 20: \n",
    "                continue\n",
    "\n",
    "            df = df.sort_values('date')\n",
    "            total_return = (df['close'].iloc[-1] - df['close'].iloc[0]) / df['close'].iloc[0]\n",
    "\n",
    "            if sector not in sector_data:\n",
    "                sector_data[sector] = []\n",
    "            sector_data[sector].append({\n",
    "                'Ticker': ticker,\n",
    "                'Return': total_return,\n",
    "                'Start_Price': df['close'].iloc[0],\n",
    "                'End_Price': df['close'].iloc[-1],\n",
    "                'Trading_Days': len(df)\n",
    "            })\n",
    "            \n",
    "            print(f\"Processed {ticker} ({sector})\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file}: {str(e)}\")\n",
    "\n",
    "    sector_performance = []\n",
    "    sector_details = {}\n",
    "    \n",
    "    for sector, stocks in sector_data.items():\n",
    "        avg_return = sum(stock['Return'] for stock in stocks) / len(stocks)\n",
    "        sector_performance.append({\n",
    "            'Sector': sector,\n",
    "            'Average_Return': avg_return,\n",
    "            'Number_of_Stocks': len(stocks)\n",
    "        })\n",
    "\n",
    "        sector_details[sector] = sorted(stocks, key=lambda x: x['Return'], reverse=True)\n",
    "\n",
    "    sector_df = pd.DataFrame(sector_performance)\n",
    "    sector_df = sector_df.sort_values('Average_Return', ascending=False)\n",
    "\n",
    "    num_sectors = len(sector_df)\n",
    "    top_n = max(1, num_sectors // 3)\n",
    "    \n",
    "    best_sectors = sector_df.head(top_n)\n",
    "    worst_sectors = sector_df.tail(top_n)\n",
    "    normal_sectors = sector_df.iloc[top_n:-top_n] if num_sectors > 2*top_n else pd.DataFrame()\n",
    "\n",
    "    plt.figure(figsize=(15, 8))\n",
    "\n",
    "    bars = plt.bar(sector_df['Sector'], sector_df['Average_Return'] * 100)\n",
    "\n",
    "    for i, bar in enumerate(bars):\n",
    "        if i < top_n:\n",
    "            bar.set_color('green')\n",
    "        elif i >= len(bars) - top_n:\n",
    "            bar.set_color('red')\n",
    "        else:\n",
    "            bar.set_color('gray')\n",
    "    \n",
    "    plt.title('Sector-wise Performance\\n'\n",
    "              f'Period: November 2023 - November 2024', pad=20)\n",
    "    plt.xlabel('Sector')\n",
    "    plt.ylabel('Average Return (%)')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.1f}%',\n",
    "                ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig(output_path / 'sector_performance.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"\\nVisualization saved as 'sector_performance.png'\")\n",
    "\n",
    "    sector_df.to_csv(output_path / 'sector_performance.csv', index=False)\n",
    "\n",
    "    with open(output_path / 'detailed_sector_analysis.txt', 'w') as f:\n",
    "        f.write(\"Detailed Sector Analysis\\n\")\n",
    "        f.write(f\"Period: {start_date.strftime('%B %Y')} - {end_date.strftime('%B %Y')}\\n\\n\")\n",
    "        \n",
    "        for sector in sector_df['Sector']:\n",
    "            f.write(f\"\\n{sector}\\n\")\n",
    "            f.write(\"=\" * 50 + \"\\n\")\n",
    "            for stock in sector_details[sector]:\n",
    "                f.write(f\"Ticker: {stock['Ticker']}\\n\")\n",
    "                f.write(f\"Return: {stock['Return']:.2%}\\n\")\n",
    "                f.write(f\"Start Price: {stock['Start_Price']:.2f}\\n\")\n",
    "                f.write(f\"End Price: {stock['End_Price']:.2f}\\n\")\n",
    "                f.write(f\"Trading Days: {stock['Trading_Days']}\\n\")\n",
    "                f.write(\"-\" * 30 + \"\\n\")\n",
    "    \n",
    "    print(\"Detailed sector analysis saved as 'detailed_sector_analysis.txt'\")\n",
    "\n",
    "    print(\"\\nSector Performance Summary:\")\n",
    "    print(\"\\nBest Performing Sectors:\")\n",
    "    print(\"=\" * 80)\n",
    "    for _, row in best_sectors.iterrows():\n",
    "        print(f\"Sector: {row['Sector']}\")\n",
    "        print(f\"Average Return: {row['Average_Return']:.2%}\")\n",
    "        print(f\"Number of Stocks: {row['Number_of_Stocks']}\")\n",
    "        print(\"-\" * 80)\n",
    "    \n",
    "    if not normal_sectors.empty:\n",
    "        print(\"\\nNormal Performing Sectors:\")\n",
    "        print(\"=\" * 80)\n",
    "        for _, row in normal_sectors.iterrows():\n",
    "            print(f\"Sector: {row['Sector']}\")\n",
    "            print(f\"Average Return: {row['Average_Return']:.2%}\")\n",
    "            print(f\"Number of Stocks: {row['Number_of_Stocks']}\")\n",
    "            print(\"-\" * 80)\n",
    "    \n",
    "    print(\"\\nWorst Performing Sectors:\")\n",
    "    print(\"=\" * 80)\n",
    "    for _, row in worst_sectors.iterrows():\n",
    "        print(f\"Sector: {row['Sector']}\")\n",
    "        print(f\"Average Return: {row['Average_Return']:.2%}\")\n",
    "        print(f\"Number of Stocks: {row['Number_of_Stocks']}\")\n",
    "        print(\"-\" * 80)\n",
    "    \n",
    "    return sector_df\n",
    "\n",
    "input_directory = r\"D:\\Stock Project\\split_by_ticker\"\n",
    "output_directory = r\"D:\\Stock Project\\sector_analysis\"\n",
    "\n",
    "sector_results = analyze_sector_performance(input_directory, output_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stock Returns Correlation Matrix from November 2023 - November 2024\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import glob\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "def analyze_stock_correlations(input_directory, output_directory):\n",
    "    output_path = Path(output_directory)\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    start_date = datetime(2023, 11, 1)\n",
    "    end_date = datetime(2024, 11, 30)\n",
    "    \n",
    "    print(f\"Analyzing correlations for period: {start_date.strftime('%B %Y')} - {end_date.strftime('%B %Y')}\")\n",
    "\n",
    "    stock_returns = {}\n",
    "\n",
    "    print(\"\\nProcessing stock data...\")\n",
    "    for file in glob.glob(str(Path(input_directory) / \"*.csv\")):\n",
    "        try:\n",
    "            df = pd.read_csv(file)\n",
    "            ticker = df['Ticker'].iloc[0]\n",
    "\n",
    "            df['date'] = pd.to_datetime(df['date'])\n",
    "            df = df[(df['date'] >= start_date) & (df['date'] <= end_date)]\n",
    "            \n",
    "            if len(df) < 20:  \n",
    "                continue\n",
    "\n",
    "            df = df.sort_values('date')\n",
    "            df['return'] = df['close'].pct_change()\n",
    "\n",
    "            stock_returns[ticker] = pd.Series(df['return'].values, index=df['date'])\n",
    "            \n",
    "            print(f\"Processed {ticker}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file}: {str(e)}\")\n",
    "\n",
    "    returns_df = pd.DataFrame(stock_returns)\n",
    "\n",
    "    correlation_matrix = returns_df.corr()\n",
    "\n",
    "    plt.figure(figsize=(20, 16))\n",
    "\n",
    "    mask = np.triu(np.ones_like(correlation_matrix), k=1)\n",
    "\n",
    "    sns.heatmap(correlation_matrix, \n",
    "                mask=mask,\n",
    "                annot=True,  \n",
    "                fmt='.2f',   \n",
    "                cmap='RdYlBu_r',  \n",
    "                square=True,\n",
    "                linewidths=0.5,\n",
    "                cbar_kws={\"shrink\": .5},\n",
    "                annot_kws={\"size\": 8})\n",
    "    \n",
    "    plt.title('Stock Returns Correlation Matrix\\n'\n",
    "              f'Period: November 2023 - November 2024', pad=20)\n",
    "\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig(output_path / 'correlation_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"\\nCorrelation heatmap saved as 'correlation_heatmap.png'\")\n",
    "\n",
    "    correlation_matrix.to_csv(output_path / 'correlation_matrix.csv')\n",
    "    print(\"Correlation matrix saved as 'correlation_matrix.csv'\")\n",
    "\n",
    "    upper_triangle = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "    highest_corr = upper_triangle.unstack()\n",
    "    highest_corr = highest_corr.sort_values(ascending=False)\n",
    "    highest_corr = highest_corr[highest_corr != 1.0]  \n",
    "    highest_corr = highest_corr[~np.isnan(highest_corr)]  \n",
    "\n",
    "    with open(output_path / 'correlation_analysis.txt', 'w') as f:\n",
    "        f.write(\"Stock Correlation Analysis\\n\")\n",
    "        f.write(f\"Period: {start_date.strftime('%B %Y')} - {end_date.strftime('%B %Y')}\\n\\n\")\n",
    "        \n",
    "        f.write(\"Top 10 Highest Correlations:\\n\")\n",
    "        f.write(\"=\" * 50 + \"\\n\")\n",
    "        for idx, corr in highest_corr.head(10).items():\n",
    "            f.write(f\"{idx[0]} - {idx[1]}: {corr:.3f}\\n\")\n",
    "        \n",
    "        f.write(\"\\nTop 10 Lowest Correlations:\\n\")\n",
    "        f.write(\"=\" * 50 + \"\\n\")\n",
    "        for idx, corr in highest_corr.tail(10).items():\n",
    "            f.write(f\"{idx[0]} - {idx[1]}: {corr:.3f}\\n\")\n",
    "\n",
    "        avg_corr = correlation_matrix.mean()\n",
    "        f.write(\"\\nAverage Correlation by Stock:\\n\")\n",
    "        f.write(\"=\" * 50 + \"\\n\")\n",
    "        for ticker, avg in avg_corr.sort_values(ascending=False).items():\n",
    "            f.write(f\"{ticker}: {avg:.3f}\\n\")\n",
    "    \n",
    "    print(\"Detailed correlation analysis saved as 'correlation_analysis.txt'\")\n",
    "\n",
    "    print(\"\\nCorrelation Summary:\")\n",
    "    print(\"\\nTop 5 Highest Correlated Pairs:\")\n",
    "    for idx, corr in highest_corr.head().items():\n",
    "        print(f\"{idx[0]} - {idx[1]}: {corr:.3f}\")\n",
    "    \n",
    "    print(\"\\nTop 5 Lowest Correlated Pairs:\")\n",
    "    for idx, corr in highest_corr.tail().items():\n",
    "        print(f\"{idx[0]} - {idx[1]}: {corr:.3f}\")\n",
    "    \n",
    "    print(\"\\nStocks with Highest Average Correlation:\")\n",
    "    for ticker, avg in avg_corr.nlargest(5).items():\n",
    "        print(f\"{ticker}: {avg:.3f}\")\n",
    "    \n",
    "    return correlation_matrix\n",
    "\n",
    "input_directory = r\"D:\\Stock Project\\split_by_ticker\"\n",
    "output_directory = r\"D:\\Stock Project\\correlation_analysis\"\n",
    "\n",
    "correlation_results = analyze_stock_correlations(input_directory, output_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Monthly Top 5 Gainers and Losers from November 2023 - November 2024\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import glob\n",
    "from datetime import datetime\n",
    "import calendar\n",
    "\n",
    "def analyze_monthly_performance(input_directory, output_directory):\n",
    "    output_path = Path(output_directory)\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    start_date = datetime(2023, 11, 1)\n",
    "    end_date = datetime(2024, 11, 30)\n",
    "    \n",
    "    print(f\"Analyzing period: {start_date.strftime('%B %Y')} - {end_date.strftime('%B %Y')}\")\n",
    "\n",
    "    monthly_data = {}\n",
    "\n",
    "    for file in glob.glob(str(Path(input_directory) / \"*.csv\")):\n",
    "        try:\n",
    "            df = pd.read_csv(file)\n",
    "            ticker = df['Ticker'].iloc[0]\n",
    "\n",
    "            df['date'] = pd.to_datetime(df['date'])\n",
    "            df = df[(df['date'] >= start_date) & (df['date'] <= end_date)]\n",
    "            \n",
    "            if len(df) < 2: \n",
    "                continue\n",
    "\n",
    "            df['month'] = df['date'].dt.to_period('M')\n",
    "            monthly_prices = df.groupby('month')['close'].agg(['first', 'last'])\n",
    "            monthly_returns = (monthly_prices['last'] - monthly_prices['first']) / monthly_prices['first']\n",
    "\n",
    "            for month, ret in monthly_returns.items():\n",
    "                if month not in monthly_data:\n",
    "                    monthly_data[month] = []\n",
    "                monthly_data[month].append({\n",
    "                    'Ticker': ticker,\n",
    "                    'Monthly_Return': ret\n",
    "                })\n",
    "            \n",
    "            print(f\"Processed {ticker}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file}: {str(e)}\")\n",
    "\n",
    "    num_months = len(monthly_data)\n",
    "    rows = (num_months + 2) // 3  \n",
    "    \n",
    "    fig = plt.figure(figsize=(20, 5*rows))\n",
    "    fig.suptitle('Monthly Top 5 Gainers and Losers\\nNovember 2023 - November 2024', \n",
    "                 fontsize=16, y=0.95)\n",
    "\n",
    "    monthly_summary = []\n",
    "\n",
    "    for idx, (month, data) in enumerate(sorted(monthly_data.items())):\n",
    "        month_df = pd.DataFrame(data)\n",
    "        month_df = month_df.sort_values('Monthly_Return', ascending=False)\n",
    "\n",
    "        gainers = month_df.head(5)\n",
    "        losers = month_df.tail(5)\n",
    "\n",
    "        for rank, (_, row) in enumerate(gainers.iterrows(), 1):\n",
    "            monthly_summary.append({\n",
    "                'Month': month.strftime('%Y-%m'),\n",
    "                'Type': 'Gainer',\n",
    "                'Rank': rank,\n",
    "                'Ticker': row['Ticker'],\n",
    "                'Return': row['Monthly_Return']\n",
    "            })\n",
    "        \n",
    "        for rank, (_, row) in enumerate(losers.iterrows(), 1):\n",
    "            monthly_summary.append({\n",
    "                'Month': month.strftime('%Y-%m'),\n",
    "                'Type': 'Loser',\n",
    "                'Rank': rank,\n",
    "                'Ticker': row['Ticker'],\n",
    "                'Return': row['Monthly_Return']\n",
    "            })\n",
    "\n",
    "        ax = plt.subplot(rows, 3, idx + 1)\n",
    "\n",
    "        plot_data = pd.concat([gainers, losers])\n",
    "        colors = ['green']*5 + ['red']*5\n",
    "\n",
    "        bars = ax.bar(range(len(plot_data)), plot_data['Monthly_Return'] * 100, color=colors)\n",
    "\n",
    "        ax.set_title(month.strftime('%B %Y'))\n",
    "        ax.set_xticks(range(len(plot_data)))\n",
    "        ax.set_xticklabels(plot_data['Ticker'], rotation=45, ha='right')\n",
    "        ax.set_ylabel('Return (%)')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                   f'{height:.1f}%',\n",
    "                   ha='center', va='bottom' if height > 0 else 'top',\n",
    "                   fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig(output_path / 'monthly_performance_dashboard.png', \n",
    "                dpi=300, bbox_inches='tight')\n",
    "    print(\"\\nDashboard saved as 'monthly_performance_dashboard.png'\")\n",
    "\n",
    "    summary_df = pd.DataFrame(monthly_summary)\n",
    "    summary_df.to_csv(output_path / 'monthly_performance_summary.csv', index=False)\n",
    "    print(\"Monthly summary saved as 'monthly_performance_summary.csv'\")\n",
    "\n",
    "    with open(output_path / 'monthly_performance_analysis.txt', 'w') as f:\n",
    "        f.write(\"Monthly Performance Analysis\\n\")\n",
    "        f.write(f\"Period: {start_date.strftime('%B %Y')} - {end_date.strftime('%B %Y')}\\n\\n\")\n",
    "        \n",
    "        for month in sorted(monthly_data.keys()):\n",
    "            f.write(f\"\\n{month.strftime('%B %Y')}\\n\")\n",
    "            f.write(\"=\" * 50 + \"\\n\")\n",
    "            \n",
    "            month_df = pd.DataFrame(monthly_data[month])\n",
    "            month_df = month_df.sort_values('Monthly_Return', ascending=False)\n",
    "            \n",
    "            f.write(\"\\nTop 5 Gainers:\\n\")\n",
    "            for _, row in month_df.head().iterrows():\n",
    "                f.write(f\"{row['Ticker']}: {row['Monthly_Return']:.2%}\\n\")\n",
    "            \n",
    "            f.write(\"\\nTop 5 Losers:\\n\")\n",
    "            for _, row in month_df.tail().iterrows():\n",
    "                f.write(f\"{row['Ticker']}: {row['Monthly_Return']:.2%}\\n\")\n",
    "            \n",
    "            f.write(\"\\nMonthly Statistics:\\n\")\n",
    "            f.write(f\"Average Return: {month_df['Monthly_Return'].mean():.2%}\\n\")\n",
    "            f.write(f\"Median Return: {month_df['Monthly_Return'].median():.2%}\\n\")\n",
    "            f.write(f\"Standard Deviation: {month_df['Monthly_Return'].std():.2%}\\n\")\n",
    "            \n",
    "            f.write(\"-\" * 50 + \"\\n\")\n",
    "    \n",
    "    print(\"Detailed analysis saved as 'monthly_performance_analysis.txt'\")\n",
    "    \n",
    "    return summary_df\n",
    "\n",
    "input_directory = r\"D:\\Stock Project\\split_by_ticker\"\n",
    "output_directory = r\"D:\\Stock Project\\monthly_performance\"\n",
    "\n",
    "monthly_results = analyze_monthly_performance(input_directory, output_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import csv files and store it in a mysql database\n",
    "import pymysql\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import glob\n",
    "\n",
    "def create_mysql_connection():\n",
    "    try:\n",
    "        connection = pymysql.connect(\n",
    "            host=\"localhost\",\n",
    "            user=\"gokuld1\",\n",
    "            password=\"guvi\",\n",
    "            database=\"stock_analysis\",\n",
    "            charset='utf8mb4',\n",
    "            ssl={'fake_flag_to_enable_tls': True},\n",
    "            ssl_verify_cert=False\n",
    "        )\n",
    "        return connection\n",
    "    except pymysql.Error as err:\n",
    "        print(f\"Error connecting to MySQL: {err}\")\n",
    "        return None\n",
    "\n",
    "def prepare_correlation_data(df):\n",
    "    melted_df = df.reset_index()\n",
    "    melted_df = pd.melt(melted_df, \n",
    "                        id_vars=melted_df.columns[0], \n",
    "                        var_name='Stock_B', \n",
    "                        value_name='Correlation')\n",
    "    melted_df.columns = ['Stock_A', 'Stock_B', 'Correlation']\n",
    "    return melted_df\n",
    "\n",
    "def check_table_contents(connection):\n",
    "    cursor = connection.cursor()\n",
    "    \n",
    "    tables = [\n",
    "        'volatility_analysis',\n",
    "        'correlation_analysis',\n",
    "        'sector_performance',\n",
    "        'monthly_performance',\n",
    "        'cumulative_returns_daily'\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\nChecking table contents at 2024-12-12 16:03:17\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for table in tables:\n",
    "        try:\n",
    "            cursor.execute(f\"SELECT COUNT(*) FROM {table}\")\n",
    "            count = cursor.fetchone()[0]\n",
    "            print(f\"Table {table}: {count} records\")\n",
    "            \n",
    "            cursor.execute(f\"SELECT * FROM {table} LIMIT 3\")\n",
    "            sample_data = cursor.fetchall()\n",
    "            if sample_data:\n",
    "                print(f\"Sample data from {table}:\")\n",
    "                for row in sample_data:\n",
    "                    print(row)\n",
    "                print()\n",
    "                \n",
    "        except pymysql.Error as err:\n",
    "            print(f\"Error checking table {table}: {err}\")\n",
    "    \n",
    "    cursor.close()\n",
    "\n",
    "def create_tables(connection):\n",
    "    cursor = connection.cursor()\n",
    "    \n",
    "    try:\n",
    "        cursor.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS cumulative_returns_daily (\n",
    "                Trading_Date DATE,\n",
    "                Ticker VARCHAR(20),\n",
    "                Close_Price FLOAT,\n",
    "                Daily_Return FLOAT,\n",
    "                Cumulative_Return FLOAT,\n",
    "                PRIMARY KEY (Trading_Date, Ticker)\n",
    "            )\n",
    "        \"\"\")\n",
    "        \n",
    "        cursor.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS volatility_analysis (\n",
    "                Ticker VARCHAR(20),\n",
    "                Daily_StdDev FLOAT,\n",
    "                Annualized_Volatility FLOAT,\n",
    "                Risk_Category VARCHAR(20),\n",
    "                PRIMARY KEY (Ticker)\n",
    "            )\n",
    "        \"\"\")\n",
    "        \n",
    "        cursor.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS correlation_analysis (\n",
    "                Stock_A VARCHAR(20),\n",
    "                Stock_B VARCHAR(20),\n",
    "                Correlation DOUBLE,\n",
    "                PRIMARY KEY (Stock_A, Stock_B)\n",
    "            )\n",
    "        \"\"\")\n",
    "        \n",
    "        cursor.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS sector_performance (\n",
    "                Sector VARCHAR(50),\n",
    "                Average_Return FLOAT,\n",
    "                Number_of_Stocks INT,\n",
    "                Performance_Ranking INT,\n",
    "                PRIMARY KEY (Sector)\n",
    "            )\n",
    "        \"\"\")\n",
    "        \n",
    "        cursor.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS monthly_performance (\n",
    "                Month_Date DATE,\n",
    "                Category VARCHAR(20),\n",
    "                Ranking INT,\n",
    "                Ticker VARCHAR(20),\n",
    "                Monthly_Return FLOAT,\n",
    "                PRIMARY KEY (Month_Date, Category, Ranking)\n",
    "            )\n",
    "        \"\"\")\n",
    "        \n",
    "        connection.commit()\n",
    "        print(\"All tables created successfully\")\n",
    "        \n",
    "    except pymysql.Error as err:\n",
    "        print(f\"Error creating tables: {err}\")\n",
    "        connection.rollback()\n",
    "    finally:\n",
    "        cursor.close()\n",
    "\n",
    "def import_csv_to_mysql(connection):\n",
    "    cursor = connection.cursor()\n",
    "    \n",
    "    folder_mappings = {\n",
    "        'correlation_analysis': ('correlation_analysis', 'correlation_matrix.csv'),\n",
    "        'monthly_performance': ('monthly_performance', 'monthly_performance_summary.csv'),\n",
    "        'sector_analysis': ('sector_performance', 'sector_performance.csv'),\n",
    "        'volatility_analysis': ('volatility_analysis', 'volatility_analysis.csv')\n",
    "    }\n",
    "    \n",
    "    base_path = Path('D:/Stock Project')\n",
    "    print(f\"\\nStarting CSV import at 2024-12-12 16:03:17\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    split_by_ticker_path = base_path / \"split_by_ticker\"\n",
    "    if split_by_ticker_path.exists():\n",
    "        print(\"Processing daily returns data...\")\n",
    "        for csv_file in split_by_ticker_path.glob(\"*.csv\"):\n",
    "            try:\n",
    "                df = pd.read_csv(csv_file)\n",
    "                df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "                mask = (df['date'] >= '2023-11-01') & (df['date'] <= '2024-11-30')\n",
    "                df = df[mask]\n",
    "                \n",
    "                if len(df) > 0:\n",
    "                    df['daily_return'] = df['close'].pct_change()\n",
    "                    df['cumulative_return'] = (1 + df['daily_return']).cumprod() - 1\n",
    "\n",
    "                    values = []\n",
    "                    for _, row in df.iterrows():\n",
    "                        values.append((\n",
    "                            row['date'].strftime('%Y-%m-%d'),\n",
    "                            row['Ticker'],\n",
    "                            row['close'],\n",
    "                            row['daily_return'] if not pd.isna(row['daily_return']) else 0,\n",
    "                            row['cumulative_return'] if not pd.isna(row['cumulative_return']) else 0\n",
    "                        ))\n",
    "                    \n",
    "                    insert_query = \"\"\"\n",
    "                        INSERT INTO cumulative_returns_daily \n",
    "                        (Trading_Date, Ticker, Close_Price, Daily_Return, Cumulative_Return)\n",
    "                        VALUES (%s, %s, %s, %s, %s)\n",
    "                        ON DUPLICATE KEY UPDATE\n",
    "                        Close_Price = VALUES(Close_Price),\n",
    "                        Daily_Return = VALUES(Daily_Return),\n",
    "                        Cumulative_Return = VALUES(Cumulative_Return)\n",
    "                    \"\"\"\n",
    "                    cursor.executemany(insert_query, values)\n",
    "                    connection.commit()\n",
    "                    print(f\"Processed daily returns for {df['Ticker'].iloc[0]}\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {csv_file.name}: {e}\")\n",
    "                connection.rollback()\n",
    "\n",
    "    for folder_name, (table_name, csv_name) in folder_mappings.items():\n",
    "        try:\n",
    "            csv_path = base_path / folder_name / csv_name\n",
    "            print(f\"Processing {csv_path}\")\n",
    "            \n",
    "            if not csv_path.exists():\n",
    "                print(f\"Warning: File not found - {csv_path}\")\n",
    "                continue\n",
    "            \n",
    "            df = pd.read_csv(csv_path)\n",
    "            \n",
    "            if table_name == 'correlation_analysis':\n",
    "                df = pd.read_csv(csv_path, index_col=0)\n",
    "                df = prepare_correlation_data(df)\n",
    "            \n",
    "            elif table_name == 'monthly_performance':\n",
    "                df['Month_Date'] = pd.to_datetime(df['Month']).dt.strftime('%Y-%m-%d')\n",
    "                df = df.rename(columns={\n",
    "                    'Return': 'Monthly_Return',\n",
    "                    'Rank': 'Ranking',\n",
    "                    'Type': 'Category'\n",
    "                }).drop('Month', axis=1)\n",
    "            \n",
    "            elif table_name == 'sector_performance':\n",
    "                df = df.rename(columns={\n",
    "                    'Rank': 'Performance_Ranking'\n",
    "                })\n",
    "                df = df.drop_duplicates(subset=['Sector'], keep='first')\n",
    "            \n",
    "            elif table_name == 'volatility_analysis':\n",
    "                df = pd.read_csv(csv_path)\n",
    "                df['Risk_Category'] = pd.qcut(df['Daily_StdDev'], \n",
    "                                           q=3, \n",
    "                                           labels=['Low', 'Medium', 'High'])\n",
    "                df = df[['Ticker', 'Daily_StdDev', 'Annualized_Volatility', 'Risk_Category']]\n",
    "            \n",
    "            columns = ', '.join(f\"`{col}`\" for col in df.columns)\n",
    "            placeholders = ', '.join(['%s'] * len(df.columns))\n",
    "            insert_query = f\"INSERT INTO {table_name} ({columns}) VALUES ({placeholders})\"\n",
    "            \n",
    "            values = [tuple(row) for row in df.values]\n",
    "            cursor.executemany(insert_query, values)\n",
    "            connection.commit()\n",
    "            \n",
    "            print(f\"Successfully imported {len(values)} records into {table_name}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error importing from {folder_name}: {e}\")\n",
    "            print(f\"DataFrame columns: {df.columns.tolist()}\")\n",
    "            connection.rollback()\n",
    "    \n",
    "    cursor.close()\n",
    "\n",
    "def main():\n",
    "    print(f\"Starting database operations at: 2024-12-12 16:03:17\")\n",
    "    \n",
    "    connection = create_mysql_connection()\n",
    "    \n",
    "    if connection:\n",
    "        try:\n",
    "            create_tables(connection)\n",
    "            import_csv_to_mysql(connection)\n",
    "            check_table_contents(connection)\n",
    "            \n",
    "        finally:\n",
    "            connection.close()\n",
    "            print(\"\\nMySQL connection closed\")\n",
    "            print(f\"Operations completed at: 2024-12-12 16:03:17\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
